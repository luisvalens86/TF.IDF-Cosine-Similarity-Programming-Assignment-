{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-mbGVb5vxeaJ",
        "7A4fCYM8xXr6",
        "knfxO9LqwzDj",
        "lNvinzatzmVw",
        "KYlt8_uwzQi-",
        "NfeTQkTGwKxk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### **TF.IDF & Cosine Similarity Programming Assignment** \n",
        "\n",
        "*By: Luis Valenzuela* \\\n",
        "*CSCI Information Retrieval* \\\n"
      ],
      "metadata": {
        "id": "-mbGVb5vxeaJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNez7x2-zwzG",
        "outputId": "1a087851-93b2-4fed-ff49-a96168d700f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data:**"
      ],
      "metadata": {
        "id": "7A4fCYM8xXr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from urllib import request\n",
        "from nltk import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import os\n",
        "import numpy as np\n",
        "from nltk.stem import *\n",
        "from nltk.stem.porter import *\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# read the files\n",
        "doc1 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a1.txt', 'r').read()\n",
        "doc2 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a2.txt', 'r').read()\n",
        "doc3 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a3.txt', 'r').read()\n",
        "doc4 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a4.txt', 'r').read()\n",
        "doc5 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a5.txt', 'r').read()\n",
        "doc6 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a6.txt', 'r').read()\n",
        "doc7 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a7.txt', 'r').read()\n",
        "doc8 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a8.txt', 'r').read()\n",
        "doc9 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a9.txt', 'r').read()\n",
        "doc10 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a10.txt', 'r').read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd71u2AizyG2",
        "outputId": "32968477-2db4-4d31-9845-2c25d4787fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Tokenization**"
      ],
      "metadata": {
        "id": "knfxO9LqwzDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the documents\n",
        "doc1_tokens = word_tokenize(doc1)\n",
        "d1 = [w.lower() for w in doc1_tokens]\n",
        "doc2_tokens = word_tokenize(doc2)\n",
        "d2 = [w.lower() for w in doc2_tokens]\n",
        "doc3_tokens = word_tokenize(doc3)\n",
        "d3 = [w.lower() for w in doc3_tokens]\n",
        "doc4_tokens = word_tokenize(doc4)\n",
        "d4 = [w.lower() for w in doc4_tokens]\n",
        "doc5_tokens = word_tokenize(doc5)\n",
        "d5 = [w.lower() for w in doc5_tokens]\n",
        "doc6_tokens = word_tokenize(doc6)\n",
        "d6 = [w.lower() for w in doc6_tokens]\n",
        "doc7_tokens = word_tokenize(doc7)\n",
        "d7 = [w.lower() for w in doc7_tokens]\n",
        "doc8_tokens = word_tokenize(doc8)\n",
        "d8 = [w.lower() for w in doc8_tokens]\n",
        "doc9_tokens = word_tokenize(doc9)\n",
        "d9 = [w.lower() for w in doc9_tokens]\n",
        "doc10_tokens = word_tokenize(doc10)\n",
        "d10 = [w.lower() for w in doc10_tokens]"
      ],
      "metadata": {
        "id": "fO80Lapr0RQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count the number of tokens in each document\n",
        "doc1_num_tokens = len(d1)\n",
        "doc2_num_tokens = len(d2)\n",
        "doc3_num_tokens = len(d3)\n",
        "doc4_num_tokens = len(d4)\n",
        "doc5_num_tokens = len(d5)\n",
        "doc6_num_tokens = len(d6)\n",
        "doc7_num_tokens = len(d7)\n",
        "doc8_num_tokens = len(d8)\n",
        "doc9_num_tokens = len(d9)\n",
        "doc10_num_tokens = len(d10)"
      ],
      "metadata": {
        "id": "5GIrCG1a0gdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count the number of unique tokens (types) in each document\n",
        "doc1_num_dist = len(set(d1))\n",
        "doc2_num_dist = len(set(d2))\n",
        "doc3_num_dist = len(set(d3))\n",
        "doc4_num_dist = len(set(d4))\n",
        "doc5_num_dist = len(set(d5))\n",
        "doc6_num_dist = len(set(d6))\n",
        "doc7_num_dist = len(set(d7))\n",
        "doc8_num_dist = len(set(d8))\n",
        "doc9_num_dist = len(set(d9))\n",
        "doc10_num_dist = len(set(d10))"
      ],
      "metadata": {
        "id": "dPOdEdpR0zoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine all tokens in the corpus\n",
        "corpus_tokens = d1+d2+d3+d4+d5+d6+d7+d8+d9+d10\n",
        "\n",
        "# count the number of tokens in the entire corpus\n",
        "corpus_num_tokens = len(corpus_tokens)\n",
        "print(f\"Collection of tokens: {corpus_num_tokens}\")\n",
        "\n",
        "# count the number of unique tokens (types) in the entire corpus\n",
        "corpus_num_dist = len(set(corpus_tokens))\n",
        "print(f\"Collection of unique tokens(types): {corpus_num_dist}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWtokhz41bGv",
        "outputId": "ccfa19d1-89ef-447f-f6fc-7b55d709cb36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collection of tokens: 24165\n",
            "Collection of unique tokens(types): 3762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokens and token types(unique tokens) in each document\n",
        "print(\"Document 1: Tokens =\", doc1_num_tokens, \", Types(unique tokens) =\", doc1_num_dist)\n",
        "print(\"Document 2: Tokens =\", doc2_num_tokens, \", Types(unique tokens) =\", doc2_num_dist)\n",
        "print(\"Document 3: Tokens =\", doc3_num_tokens, \", Types(unique tokens) =\", doc3_num_dist)\n",
        "print(\"Document 4: Tokens =\", doc4_num_tokens, \", Types(unique tokens) =\", doc4_num_dist)\n",
        "print(\"Document 5: Tokens =\", doc5_num_tokens, \", Types(unique tokens) =\", doc5_num_dist)\n",
        "print(\"Document 6: Tokens =\", doc6_num_tokens, \", Types(unique tokens) =\", doc6_num_dist)\n",
        "print(\"Document 7: Tokens =\", doc7_num_tokens, \", Types(unique tokens) =\", doc7_num_dist)\n",
        "print(\"Document 8: Tokens =\", doc8_num_tokens, \", Types(unique tokens) =\", doc8_num_dist)\n",
        "print(\"Document 9: Tokens =\", doc9_num_tokens, \", Types(unique tokens) =\", doc9_num_dist)\n",
        "print(\"Document 10: Tokens =\", doc10_num_tokens, \", Types(unique tokens) =\", doc10_num_dist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR5qTZCz2cnE",
        "outputId": "ffecc630-e019-416d-b8f4-50cab4238b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1: Tokens = 1945 , Types(unique tokens) = 623\n",
            "Document 2: Tokens = 4888 , Types(unique tokens) = 1256\n",
            "Document 3: Tokens = 2191 , Types(unique tokens) = 709\n",
            "Document 4: Tokens = 3364 , Types(unique tokens) = 973\n",
            "Document 5: Tokens = 3466 , Types(unique tokens) = 1046\n",
            "Document 6: Tokens = 1782 , Types(unique tokens) = 618\n",
            "Document 7: Tokens = 1643 , Types(unique tokens) = 598\n",
            "Document 8: Tokens = 1836 , Types(unique tokens) = 668\n",
            "Document 9: Tokens = 1602 , Types(unique tokens) = 573\n",
            "Document 10: Tokens = 1448 , Types(unique tokens) = 544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Stopword Removal**"
      ],
      "metadata": {
        "id": "lNvinzatzmVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopword removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "#d1\n",
        "f1 = [w for w in d1 if not w.lower() in stop_words]\n",
        "f1 = []  \n",
        "for w in d1:\n",
        "    if w not in stop_words:\n",
        "        f1.append(w)\n",
        "#d2\n",
        "f2 = [w for w in d2 if not w.lower() in stop_words]\n",
        "f2 = []  \n",
        "for w in d2:\n",
        "    if w not in stop_words:\n",
        "        f2.append(w)\n",
        "#d3\n",
        "f3 = [w for w in d3 if not w.lower() in stop_words]\n",
        "f3 = []  \n",
        "for w in d3:\n",
        "    if w not in stop_words:\n",
        "        f3.append(w)\n",
        "#d4\n",
        "f4 = [w for w in d4 if not w.lower() in stop_words]\n",
        "f4 = []  \n",
        "for w in d4:\n",
        "    if w not in stop_words:\n",
        "        f4.append(w)\n",
        "#d5\n",
        "f5 = [w for w in d5 if not w.lower() in stop_words]\n",
        "f5 = []  \n",
        "for w in d5:\n",
        "    if w not in stop_words:\n",
        "        f5.append(w)\n",
        "#d6\n",
        "f6 = [w for w in d6 if not w.lower() in stop_words]\n",
        "f6 = []  \n",
        "for w in d6:\n",
        "    if w not in stop_words:\n",
        "        f6.append(w)\n",
        "#d7\n",
        "f7 = [w for w in d7 if not w.lower() in stop_words]\n",
        "f7 = []  \n",
        "for w in d7:\n",
        "    if w not in stop_words:\n",
        "        f7.append(w)\n",
        "#d8\n",
        "f8 = [w for w in d8 if not w.lower() in stop_words]\n",
        "f8 = []  \n",
        "for w in d8:\n",
        "    if w not in stop_words:\n",
        "        f8.append(w)\n",
        "#d9\n",
        "f9 = [w for w in d9 if not w.lower() in stop_words]\n",
        "f9 = []  \n",
        "for w in d9:\n",
        "    if w not in stop_words:\n",
        "        f9.append(w)\n",
        "#d10\n",
        "f10 = [w for w in d10 if not w.lower() in stop_words]\n",
        "f10 = []  \n",
        "for w in d10:\n",
        "    if w not in stop_words:\n",
        "        f10.append(w)"
      ],
      "metadata": {
        "id": "Msmrn5hmYbgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count the number of tokens in each document after removing stop-words\n",
        "doc1_tkn_stwd = len(f1)\n",
        "doc2_tkn_stwd = len(f2)\n",
        "doc3_tkn_stwd = len(f3)\n",
        "doc4_tkn_stwd = len(f4)\n",
        "doc5_tkn_stwd = len(f5)\n",
        "doc6_tkn_stwd = len(f6)\n",
        "doc7_tkn_stwd = len(f7)\n",
        "doc8_tkn_stwd = len(f8)\n",
        "doc9_tkn_stwd = len(f9)\n",
        "doc10_tkn_stwd = len(f10)"
      ],
      "metadata": {
        "id": "lENmG4ADdEml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count the number of unique tokens (types) in each document after removing stop-words\n",
        "doc1_dist_stwd = len(set(f1))\n",
        "doc2_dist_stwd = len(set(f2))\n",
        "doc3_dist_stwd = len(set(f3))\n",
        "doc4_dist_stwd = len(set(f4))\n",
        "doc5_dist_stwd = len(set(f5))\n",
        "doc6_dist_stwd = len(set(f6))\n",
        "doc7_dist_stwd = len(set(f7))\n",
        "doc8_dist_stwd = len(set(f8))\n",
        "doc9_dist_stwd = len(set(f9))\n",
        "doc10_dist_stwd = len(set(f10))"
      ],
      "metadata": {
        "id": "evI-o-Vc2NXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine all tokens in the corpus after removing all the stop-words\n",
        "corp_sum_tkn_stwd = f1+f2+f3+f4+f5+f6+f7+f8+f9+f10\n",
        "\n",
        "# count the num of tokens in the entire corpus after removing the stop-words\n",
        "corp_tkn_stwd = len(corp_sum_tkn_stwd)\n",
        "print(f\"Collection of tokens after removal of stop-words: {corp_tkn_stwd}\")\n",
        "\n",
        "# count the num of unique tokens (types) in the entire corpus after removing the stop-words\n",
        "corp_dist_stwd = len(set(corp_sum_tkn_stwd))\n",
        "print(f\"Collection of unique tokens (types) after removal of stop-words: {corp_dist_stwd}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCuG3nYOkTXu",
        "outputId": "828e5e5b-ca3e-49c1-c5c8-15847ce1d4de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collection of tokens after removal of stop-words: 15526\n",
            "Collection of unique tokens (types) after removal of stop-words: 3646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokens and token types(unique tokens) in each document after stopword removal\n",
        "print(\"Doc1:(stopwords): Tokens =\", doc1_tkn_stwd, \", Types(unique tokens) =\", doc1_dist_stwd)\n",
        "print(\"Doc2:(stopwords): Tokens =\", doc2_tkn_stwd, \", Types(unique tokens) =\", doc2_dist_stwd)\n",
        "print(\"Doc3:(stopwords): Tokens =\", doc3_tkn_stwd, \", Types(unique tokens) =\", doc3_dist_stwd)\n",
        "print(\"Doc4:(stopwords): Tokens =\", doc4_tkn_stwd, \", Types(unique tokens) =\", doc4_dist_stwd)\n",
        "print(\"Doc5:(stopwords): Tokens =\", doc5_tkn_stwd, \", Types(unique tokens) =\", doc5_dist_stwd)\n",
        "print(\"Doc6:(stopwords): Tokens =\", doc6_tkn_stwd, \", Types(unique tokens) =\", doc6_dist_stwd)\n",
        "print(\"Doc7:(stopwords): Tokens =\", doc7_tkn_stwd, \", Types(unique tokens) =\", doc7_dist_stwd)\n",
        "print(\"Doc8:(stopwords): Tokens =\", doc8_tkn_stwd, \", Types(unique tokens) =\", doc8_dist_stwd)\n",
        "print(\"Doc9:(stopwords): Tokens =\", doc9_tkn_stwd, \", Types(unique tokens) =\", doc9_dist_stwd)\n",
        "print(\"Doc10:(stopwords): Tokens =\", doc10_tkn_stwd, \", Types(unique tokens) =\", doc10_dist_stwd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5xVfHzwmLxX",
        "outputId": "fbba1886-dcdf-4ab5-93a8-31a18440857a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc1:(stopwords): Tokens = 1244 , Types(unique tokens) = 545\n",
            "Doc2:(stopwords): Tokens = 3082 , Types(unique tokens) = 1152\n",
            "Doc3:(stopwords): Tokens = 1382 , Types(unique tokens) = 639\n",
            "Doc4:(stopwords): Tokens = 2196 , Types(unique tokens) = 885\n",
            "Doc5:(stopwords): Tokens = 2254 , Types(unique tokens) = 960\n",
            "Doc6:(stopwords): Tokens = 1149 , Types(unique tokens) = 543\n",
            "Doc7:(stopwords): Tokens = 1063 , Types(unique tokens) = 530\n",
            "Doc8:(stopwords): Tokens = 1201 , Types(unique tokens) = 590\n",
            "Doc9:(stopwords): Tokens = 1028 , Types(unique tokens) = 496\n",
            "Doc10:(stopwords): Tokens = 927 , Types(unique tokens) = 473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Stemming/Lemmatization**"
      ],
      "metadata": {
        "id": "KYlt8_uwzQi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "porter = nltk.PorterStemmer()\n",
        "\n",
        "porter_tok1 = [porter.stem(t) for t in f1]\n",
        "porter_tok2 = [porter.stem(t) for t in f2]\n",
        "porter_tok3 = [porter.stem(t) for t in f3]\n",
        "porter_tok4 = [porter.stem(t) for t in f4]\n",
        "porter_tok5 = [porter.stem(t) for t in f5]\n",
        "porter_tok6 = [porter.stem(t) for t in f6]\n",
        "porter_tok7 = [porter.stem(t) for t in f7]\n",
        "porter_tok8 = [porter.stem(t) for t in f8]\n",
        "porter_tok9 = [porter.stem(t) for t in f9]\n",
        "porter_tok10 = [porter.stem(t) for t in f10]\n",
        "\n",
        "\n",
        "porter_tok1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HGxfwZCuHJs",
        "outputId": "0274f791-4848-4e68-bc04-2b581075013d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gyarado',\n",
              " 'gyarado',\n",
              " '(',\n",
              " 'ギャラドス',\n",
              " ',',\n",
              " 'gyaradosu',\n",
              " ',',\n",
              " ')',\n",
              " 'pokémon',\n",
              " 'speci',\n",
              " 'nintendo',\n",
              " 'game',\n",
              " 'freak',\n",
              " \"'s\",\n",
              " 'pokémon',\n",
              " 'franchis',\n",
              " '.',\n",
              " 'creat',\n",
              " 'ken',\n",
              " 'sugimori',\n",
              " ',',\n",
              " 'gyarado',\n",
              " 'first',\n",
              " 'appear',\n",
              " 'video',\n",
              " 'game',\n",
              " 'pokémon',\n",
              " 'red',\n",
              " 'pokemon',\n",
              " 'green',\n",
              " 'subsequ',\n",
              " 'sequel',\n",
              " ',',\n",
              " 'later',\n",
              " 'appear',\n",
              " 'variou',\n",
              " 'merchandis',\n",
              " ',',\n",
              " 'spinoff',\n",
              " 'titl',\n",
              " 'anim',\n",
              " 'print',\n",
              " 'adapt',\n",
              " 'franchis',\n",
              " '.',\n",
              " 'gyarado',\n",
              " 'voic',\n",
              " 'unshō',\n",
              " 'ishizuka',\n",
              " 'japanes',\n",
              " 'english',\n",
              " 'media',\n",
              " '.',\n",
              " 'known',\n",
              " 'atroci',\n",
              " 'pokémon',\n",
              " ',',\n",
              " 'gyarado',\n",
              " 'evolv',\n",
              " 'form',\n",
              " 'magikarp',\n",
              " 'well',\n",
              " 'known',\n",
              " 'pokémon',\n",
              " 'world',\n",
              " 'fierc',\n",
              " 'temper',\n",
              " 'well',\n",
              " 'reput',\n",
              " 'caus',\n",
              " 'noth',\n",
              " 'destruct',\n",
              " 'much',\n",
              " 'work',\n",
              " 'frenzi',\n",
              " ',',\n",
              " 'calm',\n",
              " 'everyth',\n",
              " 'around',\n",
              " 'destroy',\n",
              " '.',\n",
              " 'gyarado',\n",
              " 'appear',\n",
              " 'multipl',\n",
              " 'time',\n",
              " 'anim',\n",
              " 'variou',\n",
              " 'trainer',\n",
              " 'misti',\n",
              " ',',\n",
              " 'lanc',\n",
              " ',',\n",
              " 'crasher',\n",
              " 'wake',\n",
              " ',',\n",
              " 'nurs',\n",
              " 'joy',\n",
              " '.',\n",
              " 'two',\n",
              " 'differ',\n",
              " 'gyarado',\n",
              " 'appear',\n",
              " 'pokémon',\n",
              " 'adventur',\n",
              " 'manga',\n",
              " '.',\n",
              " 'one',\n",
              " 'origin',\n",
              " 'own',\n",
              " 'misti',\n",
              " ',',\n",
              " 'trade',\n",
              " 'red',\n",
              " 'blue',\n",
              " '.',\n",
              " 'red',\n",
              " 'gyarado',\n",
              " 'own',\n",
              " 'silver',\n",
              " '.',\n",
              " 'sinc',\n",
              " 'appear',\n",
              " 'pokémon',\n",
              " 'seri',\n",
              " ',',\n",
              " 'gyarado',\n",
              " 'receiv',\n",
              " 'gener',\n",
              " 'posit',\n",
              " 'recept',\n",
              " '.',\n",
              " 'featur',\n",
              " 'sever',\n",
              " 'form',\n",
              " 'merchandis',\n",
              " ',',\n",
              " 'includ',\n",
              " 'figurin',\n",
              " ',',\n",
              " 'plush',\n",
              " 'toy',\n",
              " ',',\n",
              " 'pokémon',\n",
              " 'trade',\n",
              " 'card',\n",
              " 'game',\n",
              " '.',\n",
              " 'gyarado',\n",
              " 'describ',\n",
              " 'one',\n",
              " 'well',\n",
              " 'known',\n",
              " 'power',\n",
              " 'pokémon',\n",
              " '.',\n",
              " 'except',\n",
              " 'bounc',\n",
              " 'hurrican',\n",
              " '(',\n",
              " 'introduc',\n",
              " 'gener',\n",
              " 'iv',\n",
              " 'onward',\n",
              " ')',\n",
              " ',',\n",
              " 'learn',\n",
              " 'fli',\n",
              " 'type',\n",
              " 'move',\n",
              " '.',\n",
              " 'design',\n",
              " 'characterist',\n",
              " 'gyarado',\n",
              " 'one',\n",
              " '151',\n",
              " 'differ',\n",
              " 'design',\n",
              " 'conceiv',\n",
              " 'game',\n",
              " 'freak',\n",
              " \"'s\",\n",
              " 'charact',\n",
              " 'develop',\n",
              " 'team',\n",
              " 'final',\n",
              " 'ken',\n",
              " 'sugimori',\n",
              " 'first',\n",
              " 'gener',\n",
              " 'pocket',\n",
              " 'monster',\n",
              " 'game',\n",
              " 'red',\n",
              " 'green',\n",
              " ',',\n",
              " 'local',\n",
              " 'outsid',\n",
              " 'japan',\n",
              " 'pokémon',\n",
              " 'red',\n",
              " 'blue',\n",
              " '.',\n",
              " 'origin',\n",
              " 'call',\n",
              " '``',\n",
              " 'gyarado',\n",
              " \"''\",\n",
              " 'japanes',\n",
              " ',',\n",
              " 'nintendo',\n",
              " 'decid',\n",
              " 'give',\n",
              " 'variou',\n",
              " 'pokémon',\n",
              " 'speci',\n",
              " '``',\n",
              " 'clever',\n",
              " 'descript',\n",
              " 'name',\n",
              " \"''\",\n",
              " 'relat',\n",
              " 'appear',\n",
              " 'featur',\n",
              " 'translat',\n",
              " 'game',\n",
              " 'western',\n",
              " 'audienc',\n",
              " 'mean',\n",
              " 'make',\n",
              " 'charact',\n",
              " 'relat',\n",
              " 'american',\n",
              " 'children',\n",
              " '.',\n",
              " 'result',\n",
              " ',',\n",
              " 'speci',\n",
              " \"'\",\n",
              " 'beta',\n",
              " 'name',\n",
              " '``',\n",
              " 'skulkraken',\n",
              " \"''\",\n",
              " ',',\n",
              " 'combin',\n",
              " '``',\n",
              " 'skull',\n",
              " \"''\",\n",
              " '``',\n",
              " 'skulk',\n",
              " \"''\",\n",
              " '``',\n",
              " 'kraken',\n",
              " \"''\",\n",
              " ',',\n",
              " 'final',\n",
              " 'releas',\n",
              " 'origin',\n",
              " 'japanes',\n",
              " 'name',\n",
              " 'use',\n",
              " '.',\n",
              " 'gyarado',\n",
              " 'larg',\n",
              " 'sea',\n",
              " 'serpent',\n",
              " 'pokémon',\n",
              " 'similar',\n",
              " 'appear',\n",
              " 'dragon',\n",
              " 'seen',\n",
              " 'chines',\n",
              " 'mytholog',\n",
              " '.',\n",
              " 'concept',\n",
              " 'magikarp',\n",
              " 'evolv',\n",
              " 'gyarado',\n",
              " 'base',\n",
              " 'ancient',\n",
              " 'chines',\n",
              " 'tradit',\n",
              " 'carp',\n",
              " 'would',\n",
              " 'evolv',\n",
              " 'dragon',\n",
              " 'swim',\n",
              " 'upstream',\n",
              " '.',\n",
              " 'snakelik',\n",
              " 'bodi',\n",
              " 'larg',\n",
              " 'blue',\n",
              " ',',\n",
              " 'underbelli',\n",
              " 'pale',\n",
              " 'yellow',\n",
              " '.',\n",
              " 'four',\n",
              " 'white',\n",
              " 'fin',\n",
              " 'back',\n",
              " ',',\n",
              " 'larg',\n",
              " 'gape',\n",
              " 'mouth',\n",
              " '.',\n",
              " 'gyarado',\n",
              " 'known',\n",
              " 'fierc',\n",
              " 'temper',\n",
              " 'wanton',\n",
              " 'destruct',\n",
              " 'tendenc',\n",
              " '.',\n",
              " 'work',\n",
              " 'frenzi',\n",
              " ',',\n",
              " 'calm',\n",
              " 'everyth',\n",
              " 'around',\n",
              " 'destroy',\n",
              " ',',\n",
              " 'even',\n",
              " 'go',\n",
              " 'whole',\n",
              " 'month',\n",
              " '.',\n",
              " 'violent',\n",
              " 'natur',\n",
              " 'attribut',\n",
              " 'dramat',\n",
              " 'structur',\n",
              " 'chang',\n",
              " 'brain',\n",
              " 'undergo',\n",
              " 'evolut',\n",
              " '.',\n",
              " 'time',\n",
              " 'human',\n",
              " 'conflict',\n",
              " 'gyarado',\n",
              " 'said',\n",
              " 'appear',\n",
              " ',',\n",
              " 'burn',\n",
              " 'entir',\n",
              " 'citi',\n",
              " 'ground',\n",
              " '.',\n",
              " 'gyarado',\n",
              " 'usual',\n",
              " 'live',\n",
              " 'larg',\n",
              " 'bodi',\n",
              " 'water',\n",
              " ',',\n",
              " 'lake',\n",
              " 'pond',\n",
              " 'even',\n",
              " 'sea',\n",
              " 'ocean',\n",
              " '.',\n",
              " 'appear',\n",
              " 'video',\n",
              " 'game',\n",
              " 'gyarado',\n",
              " 'first',\n",
              " 'appear',\n",
              " 'pokémon',\n",
              " 'video',\n",
              " 'game',\n",
              " 'seri',\n",
              " 'pokémon',\n",
              " 'red',\n",
              " 'blue',\n",
              " ',',\n",
              " 'later',\n",
              " 'appear',\n",
              " 'everi',\n",
              " 'subsequ',\n",
              " 'sequel',\n",
              " '.',\n",
              " 'gyarado',\n",
              " 'found',\n",
              " 'uncommonli',\n",
              " 'fish',\n",
              " 'super',\n",
              " 'rod',\n",
              " ',',\n",
              " 'evolv',\n",
              " 'magikarp',\n",
              " 'gain',\n",
              " 'enough',\n",
              " 'experi',\n",
              " 'battl',\n",
              " '.',\n",
              " 'pokémon',\n",
              " 'gold',\n",
              " ',',\n",
              " 'silver',\n",
              " ',',\n",
              " 'crystal',\n",
              " ',',\n",
              " 'remak',\n",
              " ',',\n",
              " 'red',\n",
              " 'gyarado',\n",
              " 'found',\n",
              " 'lake',\n",
              " 'rage',\n",
              " '.',\n",
              " 'player',\n",
              " 'one',\n",
              " 'chanc',\n",
              " 'catch',\n",
              " ',',\n",
              " '(',\n",
              " 'heartgold',\n",
              " 'soulsilv',\n",
              " ',',\n",
              " 'come',\n",
              " 'back',\n",
              " 'player',\n",
              " 'defeat',\n",
              " 'elit',\n",
              " 'four',\n",
              " ')',\n",
              " 'incid',\n",
              " 'trigger',\n",
              " 'battl',\n",
              " 'team',\n",
              " 'rocket',\n",
              " 'rocket',\n",
              " 'hideout',\n",
              " 'mahogani',\n",
              " 'town',\n",
              " '.',\n",
              " 'gyarado',\n",
              " 'mention',\n",
              " 'begin',\n",
              " 'diamond',\n",
              " 'pearl',\n",
              " 'player',\n",
              " 'set',\n",
              " 'quest',\n",
              " '.',\n",
              " 'gyarado',\n",
              " 'use',\n",
              " 'mani',\n",
              " 'notabl',\n",
              " 'trainer',\n",
              " 'blue',\n",
              " ',',\n",
              " 'clair',\n",
              " ',',\n",
              " 'lanc',\n",
              " ',',\n",
              " 'wallac',\n",
              " ',',\n",
              " 'pike',\n",
              " 'queen',\n",
              " 'luci',\n",
              " ',',\n",
              " 'crasher',\n",
              " 'wake',\n",
              " ',',\n",
              " 'cyru',\n",
              " '.',\n",
              " 'gyarado',\n",
              " 'one',\n",
              " 'sever',\n",
              " 'pokémon',\n",
              " 'gain',\n",
              " 'mega',\n",
              " 'evolut',\n",
              " 'pokémon',\n",
              " 'x',\n",
              " '.',\n",
              " 'mega',\n",
              " 'evolv',\n",
              " ',',\n",
              " 'becom',\n",
              " 'water/dark',\n",
              " 'type',\n",
              " 'gain',\n",
              " 'abil',\n",
              " 'mold',\n",
              " 'breaker',\n",
              " '.',\n",
              " 'gyarado',\n",
              " 'make',\n",
              " 'mani',\n",
              " 'appear',\n",
              " 'outsid',\n",
              " 'main',\n",
              " 'seri',\n",
              " ',',\n",
              " 'includ',\n",
              " 'appear',\n",
              " 'pokémon',\n",
              " 'snap',\n",
              " ',',\n",
              " 'pokémon',\n",
              " 'trade',\n",
              " 'card',\n",
              " 'game',\n",
              " ',',\n",
              " 'pokémon',\n",
              " 'mysteri',\n",
              " 'dungeon',\n",
              " ':',\n",
              " 'blue',\n",
              " 'rescu',\n",
              " 'team',\n",
              " 'red',\n",
              " 'rescu',\n",
              " 'team',\n",
              " ',',\n",
              " 'pokémon',\n",
              " 'ranger',\n",
              " ',',\n",
              " 'pokémon',\n",
              " 'go',\n",
              " '.',\n",
              " 'pokémon',\n",
              " 'mysteri',\n",
              " 'dungeon',\n",
              " ':',\n",
              " 'explor',\n",
              " 'time',\n",
              " 'explor',\n",
              " 'dark',\n",
              " ',',\n",
              " 'gyarado',\n",
              " 'appear',\n",
              " 'miracl',\n",
              " 'sea',\n",
              " '.',\n",
              " 'attempt',\n",
              " 'take',\n",
              " 'control',\n",
              " 'phion',\n",
              " ',',\n",
              " 'halt',\n",
              " 'player',\n",
              " \"'s\",\n",
              " 'team',\n",
              " '.',\n",
              " 'poképark',\n",
              " 'wii',\n",
              " ':',\n",
              " 'pikachu',\n",
              " \"'s\",\n",
              " 'adventur',\n",
              " ',',\n",
              " 'gyarado',\n",
              " 'tri',\n",
              " 'take',\n",
              " 'beach',\n",
              " 'zone',\n",
              " 'empoleon',\n",
              " \"'s\",\n",
              " 'absenc',\n",
              " ',',\n",
              " 'host',\n",
              " 'mini-gam',\n",
              " 'call',\n",
              " '``',\n",
              " 'aqua',\n",
              " 'dash',\n",
              " \"''\",\n",
              " '.',\n",
              " 'pokémon',\n",
              " 'go',\n",
              " ',',\n",
              " 'gyarado',\n",
              " 'gener',\n",
              " 'consid',\n",
              " 'one',\n",
              " 'difficult',\n",
              " 'pokémon',\n",
              " 'obtain',\n",
              " '-',\n",
              " 'must',\n",
              " 'evolv',\n",
              " 'magikarp',\n",
              " '(',\n",
              " 'much',\n",
              " 'like',\n",
              " 'pokémon',\n",
              " 'game',\n",
              " ')',\n",
              " 'requir',\n",
              " '400',\n",
              " 'magikarp',\n",
              " 'candi',\n",
              " 'evolv',\n",
              " ',',\n",
              " 'pokémon',\n",
              " 'game',\n",
              " '.',\n",
              " 'pokkén',\n",
              " 'tournament',\n",
              " 'delux',\n",
              " 'switch',\n",
              " 'version',\n",
              " ',',\n",
              " 'gyarado',\n",
              " 'shown',\n",
              " 'magikarp',\n",
              " 'festiv',\n",
              " 'one',\n",
              " 'arena',\n",
              " 'background',\n",
              " 'charact',\n",
              " ',',\n",
              " 'spectat',\n",
              " 'battl',\n",
              " 'playabl',\n",
              " 'fighter',\n",
              " 'pokémon',\n",
              " '.',\n",
              " 'anim',\n",
              " 'anim',\n",
              " ',',\n",
              " 'gyarado',\n",
              " 'first',\n",
              " 'appear',\n",
              " 'pokémon',\n",
              " '-',\n",
              " 'choos',\n",
              " '!',\n",
              " 'swim',\n",
              " 'river',\n",
              " '.',\n",
              " 'seen',\n",
              " 'pokémon',\n",
              " 'shipwreck',\n",
              " ',',\n",
              " 'jame',\n",
              " 'kick',\n",
              " 'magikarp',\n",
              " 'frustrat',\n",
              " ',',\n",
              " 'evolv',\n",
              " 'gyarado',\n",
              " '.',\n",
              " 'misti',\n",
              " 'own',\n",
              " 'gyarado',\n",
              " '.',\n",
              " 'first',\n",
              " ',',\n",
              " 'fear',\n",
              " 'dislik',\n",
              " 'gyarado',\n",
              " 'due',\n",
              " 'traumat',\n",
              " 'experi',\n",
              " ',',\n",
              " 'manag',\n",
              " 'get',\n",
              " 'fear',\n",
              " ',',\n",
              " 'take',\n",
              " 'cerulean',\n",
              " 'citi',\n",
              " 'gym',\n",
              " ',',\n",
              " 'add',\n",
              " 'one',\n",
              " 'team',\n",
              " '.',\n",
              " 'talkin',\n",
              " \"'\",\n",
              " \"'bout\",\n",
              " 'evolut',\n",
              " 'rage',\n",
              " 'innoc',\n",
              " 'focus',\n",
              " 'red',\n",
              " 'gyarado',\n",
              " '.',\n",
              " 'destruct',\n",
              " 'rampag',\n",
              " 'lanc',\n",
              " 'captur',\n",
              " '.',\n",
              " 'gyarado',\n",
              " 'appear',\n",
              " 'gain',\n",
              " 'groudon',\n",
              " 'scuffl',\n",
              " 'legend',\n",
              " 'stop',\n",
              " 'feud',\n",
              " 'groudon',\n",
              " 'kyogr',\n",
              " '.',\n",
              " 'gyarado',\n",
              " 'appear',\n",
              " 'mani',\n",
              " 'time',\n",
              " 'ownership',\n",
              " 'variou',\n",
              " 'trainer',\n",
              " ',',\n",
              " 'crasher',\n",
              " 'wake',\n",
              " 'nurs',\n",
              " 'joy',\n",
              " '.',\n",
              " 'come',\n",
              " 'apart',\n",
              " 'dream',\n",
              " '!',\n",
              " 'shini',\n",
              " 'red',\n",
              " 'gyarado',\n",
              " 'mega',\n",
              " 'evolv',\n",
              " 'form',\n",
              " 'ownership',\n",
              " 'usag',\n",
              " 'team',\n",
              " 'flare',\n",
              " 'mastermind',\n",
              " 'lysandr',\n",
              " ',',\n",
              " 'along',\n",
              " 'lysandr',\n",
              " 'male',\n",
              " 'pyroar',\n",
              " ',',\n",
              " 'kill',\n",
              " 'form',\n",
              " 'perfect',\n",
              " 'union',\n",
              " '!',\n",
              " 'print',\n",
              " 'adapt',\n",
              " 'pokémon',\n",
              " 'adventur',\n",
              " ',',\n",
              " 'gyarado',\n",
              " 'debut',\n",
              " 'red',\n",
              " ',',\n",
              " 'green',\n",
              " '&',\n",
              " 'blue',\n",
              " 'chapter',\n",
              " 'gyarado',\n",
              " 'splash',\n",
              " '!',\n",
              " '.',\n",
              " ',',\n",
              " 'enrag',\n",
              " 'one',\n",
              " 'attack',\n",
              " 'trainer',\n",
              " ',',\n",
              " 'misti',\n",
              " ',',\n",
              " 'recaught',\n",
              " 'red',\n",
              " ',',\n",
              " 'return',\n",
              " '.',\n",
              " 'later',\n",
              " ',',\n",
              " 'misti',\n",
              " 'trade',\n",
              " 'gyarado',\n",
              " 'red',\n",
              " 'exchang',\n",
              " 'red',\n",
              " \"'s\",\n",
              " 'krabbi',\n",
              " '.',\n",
              " 'red',\n",
              " 'borrow',\n",
              " 'blue',\n",
              " \"'s\",\n",
              " 'charizard',\n",
              " 'travel',\n",
              " 'mt',\n",
              " '.',\n",
              " 'silver',\n",
              " ',',\n",
              " 'temporarili',\n",
              " 'trade',\n",
              " 'gyarado',\n",
              " 'blue',\n",
              " '.',\n",
              " 'later',\n",
              " 'appear',\n",
              " 'one',\n",
              " 'blue',\n",
              " \"'s\",\n",
              " 'pokémon',\n",
              " 'volum',\n",
              " '13',\n",
              " ',',\n",
              " 'use',\n",
              " 'entei',\n",
              " ',',\n",
              " 'later',\n",
              " 'part',\n",
              " 'reveal',\n",
              " 'team',\n",
              " 'gym',\n",
              " 'leader',\n",
              " 'faceoff',\n",
              " '.',\n",
              " 'prior',\n",
              " 'firer',\n",
              " 'leafgreen',\n",
              " 'saga',\n",
              " ',',\n",
              " 'blue',\n",
              " 'return',\n",
              " 'gyarado',\n",
              " 'red',\n",
              " '.',\n",
              " 'red',\n",
              " 'gyarado',\n",
              " 'leader',\n",
              " 'group',\n",
              " 'gyarado',\n",
              " '.',\n",
              " 'team',\n",
              " 'rocket',\n",
              " 'use',\n",
              " 'goldenrod',\n",
              " 'citi',\n",
              " 'radio',\n",
              " 'tower',\n",
              " 'disturb',\n",
              " 'pokémon',\n",
              " ',',\n",
              " 'gyarado',\n",
              " 'went',\n",
              " 'crazi',\n",
              " '.',\n",
              " 'end',\n",
              " ',',\n",
              " 'caught',\n",
              " 'silver',\n",
              " '.',\n",
              " 'battl',\n",
              " 'mask',\n",
              " 'man',\n",
              " ',',\n",
              " 'gyarado',\n",
              " 'frozen',\n",
              " 'remain',\n",
              " 'bottom',\n",
              " 'lake',\n",
              " 'rage',\n",
              " 'lt.',\n",
              " 'surg',\n",
              " 'discov',\n",
              " '.',\n",
              " 'gyarado',\n",
              " 'abl',\n",
              " 'free',\n",
              " 'lt.',\n",
              " 'surg',\n",
              " 'return',\n",
              " 'silver',\n",
              " '.',\n",
              " 'misti',\n",
              " 'own',\n",
              " 'gyarado',\n",
              " 'electr',\n",
              " 'tale',\n",
              " 'pikachu',\n",
              " 'manga',\n",
              " '.',\n",
              " 'use',\n",
              " 'ash',\n",
              " 'gym',\n",
              " 'battl',\n",
              " '.',\n",
              " 'recept',\n",
              " 'sinc',\n",
              " 'appear',\n",
              " 'pokémon',\n",
              " 'seri',\n",
              " ',',\n",
              " 'gyarado',\n",
              " 'receiv',\n",
              " 'gener',\n",
              " 'posit',\n",
              " 'recept',\n",
              " '.',\n",
              " 'featur',\n",
              " 'sever',\n",
              " 'form',\n",
              " 'merchandis',\n",
              " ',',\n",
              " 'includ',\n",
              " 'figurin',\n",
              " ',',\n",
              " 'plush',\n",
              " 'toy',\n",
              " ',',\n",
              " 'pokémon',\n",
              " 'trade',\n",
              " 'card',\n",
              " 'game',\n",
              " '.',\n",
              " 'though',\n",
              " 'gamesradar',\n",
              " 'describ',\n",
              " 'magikarp',\n",
              " '``',\n",
              " '[',\n",
              " ']',\n",
              " 'ultim',\n",
              " 'useless',\n",
              " 'pokémon',\n",
              " \"''\",\n",
              " ',',\n",
              " 'describ',\n",
              " 'gyarado',\n",
              " 'one',\n",
              " '``',\n",
              " 'well-known',\n",
              " \"''\",\n",
              " 'charact',\n",
              " 'seri',\n",
              " '.',\n",
              " 'also',\n",
              " 'featur',\n",
              " '``',\n",
              " 'pokémon',\n",
              " 'week',\n",
              " \"''\",\n",
              " '.',\n",
              " 'discuss',\n",
              " 'shini',\n",
              " 'pokémon',\n",
              " ',',\n",
              " 'gamesradar',\n",
              " 'use',\n",
              " 'gyarado',\n",
              " \"'\",\n",
              " 'shini',\n",
              " 'form',\n",
              " 'exampl',\n",
              " 'dramat',\n",
              " 'chang',\n",
              " 'appear',\n",
              " '.',\n",
              " 'later',\n",
              " 'call',\n",
              " '``',\n",
              " 'weird',\n",
              " 'water/fli',\n",
              " 'type',\n",
              " ',',\n",
              " 'abl',\n",
              " 'learn',\n",
              " 'fli',\n",
              " 'move',\n",
              " '(',\n",
              " 'except',\n",
              " 'bounc',\n",
              " ')',\n",
              " \"''\",\n",
              " '.',\n",
              " 'wire',\n",
              " \"'s\",\n",
              " 'john',\n",
              " 'mix',\n",
              " 'meyer',\n",
              " 'describ',\n",
              " '``',\n",
              " 'classic',\n",
              " 'powerhous',\n",
              " \"''\",\n",
              " ',',\n",
              " 'gamespi',\n",
              " 'editor',\n",
              " 'justin',\n",
              " 'leeper',\n",
              " 'call',\n",
              " 'gyarado',\n",
              " 'choic',\n",
              " '``',\n",
              " 'macho',\n",
              " 'gamer',\n",
              " \"''\",\n",
              " '.',\n",
              " 'ign',\n",
              " 'call',\n",
              " '``',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply stemmer: Using Porter Stemmer\n",
        "# How many terms (size of vocabulary) are left in each document after\n",
        "\n",
        "ps1 = len(set(porter_tok1))\n",
        "ps2 = len(set(porter_tok2))\n",
        "ps3 = len(set(porter_tok3))\n",
        "ps4 = len(set(porter_tok4))\n",
        "ps5 = len(set(porter_tok5))\n",
        "ps6 = len(set(porter_tok6))\n",
        "ps7 = len(set(porter_tok7))\n",
        "ps8 = len(set(porter_tok8))\n",
        "ps9 = len(set(porter_tok9))\n",
        "ps10 = len(set(porter_tok10))"
      ],
      "metadata": {
        "id": "x4x7i2CvFDsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many terms after applying Porter Stemmer \n",
        "print(\"Document 1: Terms (size of vocab) after porter =\", ps1)\n",
        "print(\"Document 2: Terms (size of vocab) after porter =\", ps2)\n",
        "print(\"Document 3: Terms (size of vocab) after porter =\", ps3)\n",
        "print(\"Document 4: Terms (size of vocab) after porter =\", ps4)\n",
        "print(\"Document 5: Terms (size of vocab) after porter =\", ps5)\n",
        "print(\"Document 6: Terms (size of vocab) after porter =\", ps6)\n",
        "print(\"Document 7: Terms (size of vocab) after porter =\", ps7)\n",
        "print(\"Document 8: Terms (size of vocab) after porter =\", ps8)\n",
        "print(\"Document 9: Terms (size of vocab) after porter =\", ps9)\n",
        "print(\"Document 10: Terms (size of vocab) after porter =\", ps10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xj9rqAhfRwWE",
        "outputId": "f045854c-0992-416e-e671-9020706a25ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1: Terms (size of vocab) after porter = 493\n",
            "Document 2: Terms (size of vocab) after porter = 985\n",
            "Document 3: Terms (size of vocab) after porter = 573\n",
            "Document 4: Terms (size of vocab) after porter = 770\n",
            "Document 5: Terms (size of vocab) after porter = 829\n",
            "Document 6: Terms (size of vocab) after porter = 498\n",
            "Document 7: Terms (size of vocab) after porter = 487\n",
            "Document 8: Terms (size of vocab) after porter = 522\n",
            "Document 9: Terms (size of vocab) after porter = 444\n",
            "Document 10: Terms (size of vocab) after porter = 432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combine all tokens in the corpus after stemming\n",
        "ps_token = porter_tok1 + porter_tok2 + porter_tok3 + porter_tok4 + porter_tok5 + porter_tok6 + porter_tok7 + porter_tok8 + porter_tok9 + porter_tok10\n",
        "\n",
        "# total vocabulary size for the entire corpus after stemming\n",
        "port_stemmer = len(set(ps_token))\n",
        "print(f\"Porter Stemmer vocab size of collection: {port_stemmer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RobAfFJwblCD",
        "outputId": "57e8d5ca-8469-4b85-d3b4-9c4bcec874b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer vocab size of collection: 2878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Compute TF-IDF**"
      ],
      "metadata": {
        "id": "NfeTQkTGwKxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#os.getcwd()\n",
        "os.chdir('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/')\n",
        "\n",
        "print(os.getcwd())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fGjRXQ_Hk52",
        "outputId": "9ffab095-5837-4ab4-b9fa-73748a8e3fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# opening text file \n",
        "\n",
        "a1 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a1.txt', 'r')\n",
        "a2 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a2.txt', 'r')\n",
        "a3 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a3.txt', 'r')\n",
        "a4 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a4.txt', 'r')\n",
        "a5 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a5.txt', 'r')\n",
        "a6 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a6.txt', 'r')\n",
        "a7 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a7.txt', 'r')\n",
        "a8 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a8.txt', 'r')\n",
        "a9 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a9.txt', 'r')\n",
        "a10 = open('/content/drive/MyDrive/DTSC_IR/ColabNotebooks/Data/a10.txt', 'r')\n",
        "\n",
        "docu1 = a1.read()\n",
        "docu2 = a2.read()\n",
        "docu3 = a3.read()\n",
        "docu4 = a4.read()\n",
        "docu5 = a5.read()\n",
        "docu6 = a6.read()\n",
        "docu7 = a7.read()\n",
        "docu8 = a8.read()\n",
        "docu9 = a9.read()\n",
        "docu10 = a10.read()\n",
        "\n",
        "docs = [];\n",
        "docs.append(docu1);\n",
        "docs.append(docu2);\n",
        "docs.append(docu3);\n",
        "docs.append(docu4);\n",
        "docs.append(docu5);\n",
        "docs.append(docu6);\n",
        "docs.append(docu7);\n",
        "docs.append(docu8);\n",
        "docs.append(docu9);\n",
        "docs.append(docu10);\n"
      ],
      "metadata": {
        "id": "bV2yrdNdVvgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createVocab(docList):\n",
        "        vocab = {}\n",
        "        for doc in docList:\n",
        "                print(doc)\n",
        "                doc = doc.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "                words = word_tokenize(doc.lower())\n",
        "                for word in words:\n",
        "                        if (word in vocab.keys()):\n",
        "                                vocab[word] = vocab[word] + 1\n",
        "                        else:\n",
        "                                vocab[word] = 1\n",
        "        return vocab\n",
        "\n",
        "\n",
        "#vocab = createVocab(docs)\n"
      ],
      "metadata": {
        "id": "MHLjKYqXZ4n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute document term matrix as well idf for each term\n",
        "\n",
        "termDict = {}\n",
        "\n",
        "docsTFMat = np.zeros((len(docs), len(vocab)))\n",
        "\n",
        "docsIdfMat = np.zeros((len(vocab), len(docs)))\n",
        "\n",
        "docTermDf = pd.DataFrame(docsTFMat, columns=sorted(vocab.keys()))\n",
        "docCount = 0\n",
        "for doc in docs:\n",
        "        doc = doc.translate(str.maketrans('', '', string.punctuation))\n",
        "        words = word_tokenize(doc.lower())\n",
        "        for word in words:\n",
        "                if (word in vocab.keys()):\n",
        "                        docTermDf[word][docCount] = docTermDf[word][docCount] + 1\n",
        "\n",
        "        docCount = docCount + 1"
      ],
      "metadata": {
        "id": "FrQGy-mUa-sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Computed idf for each word in vocab\n",
        "idfDict={}\n",
        "\n",
        "for column in docTermDf.columns:\n",
        "    idfDict[column]= np.log((len(docs) +1 )/(1+ (docTermDf[column] != 0).sum()))+1\n",
        "    \n",
        "    \n",
        "#compute tf.idf matrix\n",
        "docsTfIdfMat = np.zeros((len(docs),len(vocab)))\n",
        "docTfIdfDf = pd.DataFrame(docsTfIdfMat ,columns=sorted(vocab.keys()))"
      ],
      "metadata": {
        "id": "M66oLeyIbCAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docCount = 0\n",
        "for doc in docs:\n",
        "    for key in idfDict.keys():\n",
        "        docTfIdfDf[key][docCount] = docTermDf[key][docCount] * idfDict[key]\n",
        "    docCount = docCount +1 \n",
        "    \n",
        "print(docTfIdfDf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1SvnNXkbJLz",
        "outputId": "4f1df1a5-07cc-4778-9834-1f0e7cfe47f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         10       100      1000  100yards      10th       116       117  \\\n",
            "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "2  0.000000  0.000000  0.000000  0.000000  2.299283  0.000000  0.000000   \n",
            "3  3.576915  0.000000  0.000000  0.000000  2.299283  0.000000  0.000000   \n",
            "4  0.000000  2.704748  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "5  1.788457  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "6  1.788457  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "7  3.576915  0.000000  0.000000  0.000000  0.000000  2.704748  2.704748   \n",
            "8  0.000000  0.000000  0.000000  2.704748  0.000000  0.000000  0.000000   \n",
            "9  0.000000  0.000000  5.409496  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "       11th        13       132  ...     ギャラドス      ゲンガー       サトシ      トゲピー  \\\n",
            "0  2.704748  2.299283  0.000000  ...  2.704748  0.000000  0.000000  0.000000   \n",
            "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  2.704748  0.000000   \n",
            "2  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "3  0.000000  2.299283  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "4  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "5  0.000000  0.000000  2.704748  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "6  0.000000  0.000000  0.000000  ...  0.000000  2.704748  0.000000  0.000000   \n",
            "7  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "8  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  2.704748   \n",
            "9  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "       ニャース       プリン     ミュウツー     リザードン     ルージュラ     覚醒した姿  \n",
            "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "2  0.000000  2.704748  0.000000  0.000000  0.000000  0.000000  \n",
            "3  0.000000  0.000000  0.000000  2.704748  0.000000  0.000000  \n",
            "4  0.000000  0.000000  2.704748  0.000000  0.000000  2.704748  \n",
            "5  2.704748  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "6  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "7  0.000000  0.000000  0.000000  0.000000  2.704748  0.000000  \n",
            "8  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "9  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[10 rows x 3792 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Use TfidfVectorizer to perfom the same\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "vectorizer = TfidfVectorizer(analyzer='word',norm=None, use_idf=True,smooth_idf=True)\n",
        "tfIdfMat  = vectorizer.fit_transform(docs)\n",
        "\n",
        "feature_names = sorted(vectorizer.get_feature_names_out())\n",
        "\n",
        "docList=['Doc 1','Doc 2','Doc 3','Doc 4', 'Doc 5','Doc 6','Doc 7','Doc 8', 'Doc 9', 'Doc10']\n",
        "\n",
        "skDocsTfIdfdf = pd.DataFrame(tfIdfMat.todense(),index=sorted(docList),  columns=feature_names)\n",
        "print(skDocsTfIdfdf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ5NEFxibSCf",
        "outputId": "c14fdea6-6f31-4777-d10f-94d529762d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            000        10       100      10th       116       117      11th  \\\n",
            "Doc 1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  2.704748   \n",
            "Doc 2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "Doc 3  0.000000  0.000000  0.000000  2.299283  0.000000  0.000000  0.000000   \n",
            "Doc 4  0.000000  3.576915  0.000000  2.299283  0.000000  0.000000  0.000000   \n",
            "Doc 5  0.000000  0.000000  2.299283  0.000000  0.000000  0.000000  0.000000   \n",
            "Doc 6  0.000000  1.788457  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "Doc 7  0.000000  1.788457  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "Doc 8  0.000000  3.576915  0.000000  0.000000  2.704748  2.704748  0.000000   \n",
            "Doc 9  0.000000  0.000000  2.299283  0.000000  0.000000  0.000000  0.000000   \n",
            "Doc10  5.409496  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "             13        15       151  ...      ゲンガー       サトシ   サトシ頂上決戦  \\\n",
            "Doc 1  2.011601  0.000000  1.606136  ...  0.000000  0.000000  0.000000   \n",
            "Doc 2  0.000000  0.000000  0.000000  ...  0.000000  2.704748  2.704748   \n",
            "Doc 3  0.000000  0.000000  4.818407  ...  0.000000  0.000000  0.000000   \n",
            "Doc 4  2.011601  0.000000  1.606136  ...  0.000000  0.000000  0.000000   \n",
            "Doc 5  0.000000  2.299283  0.000000  ...  0.000000  0.000000  0.000000   \n",
            "Doc 6  0.000000  0.000000  1.606136  ...  0.000000  0.000000  0.000000   \n",
            "Doc 7  0.000000  2.299283  0.000000  ...  2.704748  0.000000  0.000000   \n",
            "Doc 8  2.011601  0.000000  1.606136  ...  0.000000  0.000000  0.000000   \n",
            "Doc 9  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
            "Doc10  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
            "\n",
            "           トゲピー      ニャース       プリン     ミュウツー     リザードン     ルージュラ     覚醒した姿  \n",
            "Doc 1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "Doc 2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "Doc 3  0.000000  0.000000  2.704748  0.000000  0.000000  0.000000  0.000000  \n",
            "Doc 4  0.000000  0.000000  0.000000  0.000000  2.704748  0.000000  0.000000  \n",
            "Doc 5  0.000000  0.000000  0.000000  2.704748  0.000000  0.000000  2.704748  \n",
            "Doc 6  0.000000  2.704748  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "Doc 7  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "Doc 8  0.000000  0.000000  0.000000  0.000000  0.000000  2.704748  0.000000  \n",
            "Doc 9  2.704748  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "Doc10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[10 rows x 3662 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BO8fSGyS6jdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5: Compute the Cosine Similarity**"
      ],
      "metadata": {
        "id": "Vpy-Ey2P6l01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compute cosine similarity\n",
        "csim = cosine_similarity(tfIdfMat,tfIdfMat)\n",
        "\n",
        "csimDf = pd.DataFrame(csim,index=sorted(docList),columns=sorted(docList))\n",
        "csimDf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "yj0LZZn_fu3U",
        "outputId": "1a9cc335-4f86-4a7f-a975-f98370c135f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Doc 1     Doc 2     Doc 3     Doc 4     Doc 5     Doc 6     Doc 7  \\\n",
              "Doc 1  1.000000  0.559888  0.489070  0.526331  0.478081  0.458915  0.471176   \n",
              "Doc 2  0.559888  1.000000  0.623434  0.731642  0.651964  0.615041  0.603466   \n",
              "Doc 3  0.489070  0.623434  1.000000  0.571459  0.540526  0.503345  0.514032   \n",
              "Doc 4  0.526331  0.731642  0.571459  1.000000  0.589061  0.536480  0.550015   \n",
              "Doc 5  0.478081  0.651964  0.540526  0.589061  1.000000  0.513397  0.515867   \n",
              "Doc 6  0.458915  0.615041  0.503345  0.536480  0.513397  1.000000  0.483944   \n",
              "Doc 7  0.471176  0.603466  0.514032  0.550015  0.515867  0.483944  1.000000   \n",
              "Doc 8  0.435677  0.584838  0.488647  0.516831  0.491023  0.464084  0.470691   \n",
              "Doc 9  0.380022  0.463714  0.415236  0.425306  0.401491  0.385769  0.390855   \n",
              "Doc10  0.488684  0.617718  0.528450  0.561474  0.521422  0.498606  0.503488   \n",
              "\n",
              "          Doc 8     Doc 9     Doc10  \n",
              "Doc 1  0.435677  0.380022  0.488684  \n",
              "Doc 2  0.584838  0.463714  0.617718  \n",
              "Doc 3  0.488647  0.415236  0.528450  \n",
              "Doc 4  0.516831  0.425306  0.561474  \n",
              "Doc 5  0.491023  0.401491  0.521422  \n",
              "Doc 6  0.464084  0.385769  0.498606  \n",
              "Doc 7  0.470691  0.390855  0.503488  \n",
              "Doc 8  1.000000  0.363024  0.470007  \n",
              "Doc 9  0.363024  1.000000  0.403182  \n",
              "Doc10  0.470007  0.403182  1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-249ddf76-f92e-4ba4-bf72-48db463ccb74\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Doc 1</th>\n",
              "      <th>Doc 2</th>\n",
              "      <th>Doc 3</th>\n",
              "      <th>Doc 4</th>\n",
              "      <th>Doc 5</th>\n",
              "      <th>Doc 6</th>\n",
              "      <th>Doc 7</th>\n",
              "      <th>Doc 8</th>\n",
              "      <th>Doc 9</th>\n",
              "      <th>Doc10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Doc 1</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.559888</td>\n",
              "      <td>0.489070</td>\n",
              "      <td>0.526331</td>\n",
              "      <td>0.478081</td>\n",
              "      <td>0.458915</td>\n",
              "      <td>0.471176</td>\n",
              "      <td>0.435677</td>\n",
              "      <td>0.380022</td>\n",
              "      <td>0.488684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc 2</th>\n",
              "      <td>0.559888</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.623434</td>\n",
              "      <td>0.731642</td>\n",
              "      <td>0.651964</td>\n",
              "      <td>0.615041</td>\n",
              "      <td>0.603466</td>\n",
              "      <td>0.584838</td>\n",
              "      <td>0.463714</td>\n",
              "      <td>0.617718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc 3</th>\n",
              "      <td>0.489070</td>\n",
              "      <td>0.623434</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.571459</td>\n",
              "      <td>0.540526</td>\n",
              "      <td>0.503345</td>\n",
              "      <td>0.514032</td>\n",
              "      <td>0.488647</td>\n",
              "      <td>0.415236</td>\n",
              "      <td>0.528450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc 4</th>\n",
              "      <td>0.526331</td>\n",
              "      <td>0.731642</td>\n",
              "      <td>0.571459</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.589061</td>\n",
              "      <td>0.536480</td>\n",
              "      <td>0.550015</td>\n",
              "      <td>0.516831</td>\n",
              "      <td>0.425306</td>\n",
              "      <td>0.561474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc 5</th>\n",
              "      <td>0.478081</td>\n",
              "      <td>0.651964</td>\n",
              "      <td>0.540526</td>\n",
              "      <td>0.589061</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.513397</td>\n",
              "      <td>0.515867</td>\n",
              "      <td>0.491023</td>\n",
              "      <td>0.401491</td>\n",
              "      <td>0.521422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc 6</th>\n",
              "      <td>0.458915</td>\n",
              "      <td>0.615041</td>\n",
              "      <td>0.503345</td>\n",
              "      <td>0.536480</td>\n",
              "      <td>0.513397</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.483944</td>\n",
              "      <td>0.464084</td>\n",
              "      <td>0.385769</td>\n",
              "      <td>0.498606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc 7</th>\n",
              "      <td>0.471176</td>\n",
              "      <td>0.603466</td>\n",
              "      <td>0.514032</td>\n",
              "      <td>0.550015</td>\n",
              "      <td>0.515867</td>\n",
              "      <td>0.483944</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.470691</td>\n",
              "      <td>0.390855</td>\n",
              "      <td>0.503488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc 8</th>\n",
              "      <td>0.435677</td>\n",
              "      <td>0.584838</td>\n",
              "      <td>0.488647</td>\n",
              "      <td>0.516831</td>\n",
              "      <td>0.491023</td>\n",
              "      <td>0.464084</td>\n",
              "      <td>0.470691</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.363024</td>\n",
              "      <td>0.470007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc 9</th>\n",
              "      <td>0.380022</td>\n",
              "      <td>0.463714</td>\n",
              "      <td>0.415236</td>\n",
              "      <td>0.425306</td>\n",
              "      <td>0.401491</td>\n",
              "      <td>0.385769</td>\n",
              "      <td>0.390855</td>\n",
              "      <td>0.363024</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.403182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc10</th>\n",
              "      <td>0.488684</td>\n",
              "      <td>0.617718</td>\n",
              "      <td>0.528450</td>\n",
              "      <td>0.561474</td>\n",
              "      <td>0.521422</td>\n",
              "      <td>0.498606</td>\n",
              "      <td>0.503488</td>\n",
              "      <td>0.470007</td>\n",
              "      <td>0.403182</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-249ddf76-f92e-4ba4-bf72-48db463ccb74')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-249ddf76-f92e-4ba4-bf72-48db463ccb74 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-249ddf76-f92e-4ba4-bf72-48db463ccb74');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "240pfXvZ7vWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thank You!"
      ],
      "metadata": {
        "id": "lMe7I9yc668A"
      }
    }
  ]
}